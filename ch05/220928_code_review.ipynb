{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7 오차역전파법 구현\n",
    "\n",
    "5.7.1 신경망 학습의 overview\n",
    "    A. 전제: 신경망에 적용 가능한 가중치와 편향 존재, 이 수치를 조정하는 과정을 '학습'이라고 지칭\n",
    "\n",
    "    B. 미니배치: 훈련 데이터 중 일부 추출, 미니배치의 손실 함수 값을 줄이는 것을 목표로 함\n",
    "    C. 기울기 산출: 각 가중치 매개변수의 기울기 연산-> 손실 함수의 값을 가장 작게 하는 방향 제시\n",
    "    D. 매개변수 갱신: 기울기 방향으로 '아주 조금' 갱신\n",
    "    E. 반복: B~E 단계 반복\n",
    "\n",
    "    - 이 중, 오차역전파법은 C. 기울기 산출에서 등장\n",
    "    - 수치 미분 사용: 구현 용이, but 긴 계산 시간\n",
    "    - or 오차역전파법: 효율적으로 연산 가능\n",
    "\n",
    "5.7.2 오차역전파법을 적용한 신경망 구현\n",
    "    - 2층 신경망 구현\n",
    "\n",
    "    - 변수\n",
    "        A. params: 딕셔너리, 신경망의 매개변수(W: 가중치, b: 편향)\n",
    "        B. layers: 순서가 있는 딕셔너리, 신경망의 계층(Affine)\n",
    "        C. lastLayer: 신경망의 마지막 계층, 예시에서는 SoftmaxWithLoss 계층 활용\n",
    "\n",
    "    - 메서드\n",
    "        A. __init__: 초기화, 인수(입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일)\n",
    "        B. predict(self.x): 예측(추론) 수행, x: 이미지 데이터\n",
    "        C. loss(Self.x, t): 손실함수 값 연산, t: 정답 레이블\n",
    "        D. accuracy(self.x, t): 정확도\n",
    "        E. numerical_gradient(self.x, t): 가중치 매개변수의 기울기를 수치 미분 방식으로 연산\n",
    "        F. gradient(self.x, t): 가중치 매개변수의 기울기를 오차역전파법으로 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None        # 손실\n",
    "        self.y= None            # softmax의 출력\n",
    "        self.t = None           # 정답 레이블(원-핫 벡터)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    # 역전파의 경우, 전파하는 값을 배치의 수로 나누어 데이터 1개당 오차를 앞 계층으로 전파\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict             # 순서가 있는 딕셔너리\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        #계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lasyLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        # return 뒤에 이렇게 두면 인식이 되나여,,? 교재 오타인감,,\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)            # 순전파 때, 추가한 순서대로 각 계층의 forward 메서드 호출\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lasyLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(X)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lasyLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)         # 역전파도 계층 역순으로 호출\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7.3 오차역전파법으로 구한 기울기 검증\n",
    "    - 매개변수가 많아도 효율적으로 연산할 수 있음\n",
    "    - but, 수치 미분도 오차역전파법을 정확히 구현했는지 확인하기 위해 필요\n",
    "    == 기울기 확인(gradient check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locale import normalize\n",
    "from operator import ne\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균 연산\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수치 미분과 오차역전파법의 결과 오차가 0이 되는 경우 드뭄\n",
    "== 계산의 정밀도가 유한하기 때문(32bit 부동소수점과 같이)\n",
    "\n",
    "- 오차가 큰 경우, 오차역전파법 구현을 다시 체크해야"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28435c4127f67b7607031f32967fe3943e6f69f1a5ec9adab2b1e0962cf71e07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
