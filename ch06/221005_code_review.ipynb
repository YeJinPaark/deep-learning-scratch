{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.3 ReLU를 사용할 때의 가중치 초깃값\n",
    "    - Xavier 초깃값: 활성화 함수가 선형인 것이 전제\n",
    "    - 선형 예: sigmoid 함수, tanh 함수(좌우 대칭==중앙 부근이 선형인 함수)\n",
    "    - 표준편차가 (1/n)^(1/2)\n",
    "\n",
    "    - He 초깃값: ReLU에 특화된 초깃값\n",
    "    - 앞 계층의 노드가 n개일 때, 표준편차가 (2/n)^(1/2)인 정규분포 사용\n",
    "    == '음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다'\n",
    "\n",
    "    (비교)\n",
    "    A. std=0.01: 각 층의 활성화값들이 매우 작음\n",
    "    == 신경망에 아주 작은 데이터가 흐름 == 역전파 때 가중치의 기울기가 작아짐\n",
    "    -> 가중치가 거의 갱신되지 않음\n",
    "    B. Xavier 초깃값: 층이 깊어지면서 치우침이 조금씩 커짐\n",
    "    == 층이 깊어지면서 활성화값들의 치우침도 커짐 == 학습 시, 기울기 소실 문제를 일으킴\n",
    "    C. He 초깃값: 모든 층에서 균일하게 분포\n",
    "    == 층이 깊어져도 분표가 균일\n",
    "\n",
    "    D. 결론\n",
    "    - 활성화함수로\n",
    "        - ReLU 사용 시, He 초깃값\n",
    "        - sigmoid, tanh 등 S자 모양 곡선 함수 사용 시, Xavier 초깃값 사용\n",
    "\n",
    "6.2.4 MNIST 데이터셋으로 가중치 초깃값 비교\n",
    "    - He 초깃값이 학습 진도가 조금 더 빠름\n",
    "    - 가중치의 초깃값에 따라 신경망 학습의 성패가 갈림\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 배치 정규화\n",
    "    - 각 층이 활성화를 적당히 퍼뜨리도록 '강제'한다는 아이디어에서 출발\n",
    "\n",
    "6.3.1 배치 정규화 알고리즘\n",
    "    - 장점\n",
    "        A. 학습 속도 개선\n",
    "        B. 초깃값에 크게 의존하지 않음\n",
    "        C.오버피팅 억제-> 드롭아웃 등의 필요성 감소\n",
    "\n",
    "    - 작동: 데이터 분포를 정규화하는 '배치 정규화 계층'을 신경망에 삽입\n",
    "    - 학습 시, 미니배치를 단위로 정규화\n",
    "    == 데이터 분포가 평균 0, 분산 1이 되도록 정규화\n",
    "    (정규화 과정에서, 분모에 아주 작은 10e-7와 같은 값을 분산에 더해 연산-> 0으로 나누는 사태 예방)\n",
    "    - 활성화 함수의 앞 혹은 뒤에 배치\n",
    "\n",
    "    - 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대(scale)와 이동(shift) 변환 수행\n",
    "    - scale 파라미터는 1, shift 파라미터는 0부터 시작하면서, 학습 시 조정해나감\n",
    "    == 1배 확대 + 0만큼 이동 == 처음에는 원본에서 시작한다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 바른 학습\n",
    "    - 오버피팅: 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에 제대로 대응하지 못하는 상태\n",
    "\n",
    "6.4.1 오버피팅\n",
    "    - 발생 케이스\n",
    "        A. 매개변수가 많고, 표현력이 높은 모델\n",
    "        B. 훈련 데이터가 적은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수 조정\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\n",
    "optimizer = SGD(lr=0.01)    # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_Size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.accuracy(x_train, t_train)\n",
    "    text_acc = network.accuracy(x_test, t_test)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    epoch_cnt += 1\n",
    "    if epoch_cnt >= max_epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 데이터의 경우, 100 에폭 이후 정확도 거의 100%dp ehekf\n",
    "- 시험 데이터의 경우, 훈련 데이터 정확도와 큰 차이\n",
    "== 훈련 데이터에만 적응(fitting)한 결과"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28435c4127f67b7607031f32967fe3943e6f69f1a5ec9adab2b1e0962cf71e07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
