{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.5 적절한 하이퍼파라미터 값 찾기</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망에서의 하이퍼파라미터: 각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률, 가중치 감소 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.5.1 검증 데이터</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `훈련 데이터`: 매개변수(가중치와 편향)의 학습에 이용  \n",
    "- `검증 데이터`: 하이퍼파라미터의 성능 평가  \n",
    "\n",
    "- `시험 데이터`: 범용 성능 평가, 이상적으로 마지막에 한 번만 이용  \n",
    "$\\equiv$ 훈련 데이터에만 지나치게 적응되어있지(오버피팅) 않은지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시험 데이터로 하이퍼파라미터 성능 평가 X  \n",
    "\n",
    "    $\\equiv$ 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문  \n",
    "    $\\equiv$ 하이퍼파라미터 값이 시험 데이터에만 적합하도록 조정되기 때문  \n",
    "\n",
    "    $\\equiv$ 다른 데이터에 적응하지 못하니 범용 성능이 떨어질 가능성 존재\n",
    "\n",
    "- `검증 데이터`: 하이퍼파라미터 조정용 데이터(적절성 평가)  \n",
    "    - 미리 훈련, 검증, 시험 데이터로 분류되어 있거나  \n",
    "    - 훈련과 시험 데이터로 분류되어 있어 $\\rightarrow$ 훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리해둠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "# 입력 데이터와 정답 레이블을 뒤섞음(훈련 데이터 섞음)-> 데이터 셋 안에 치우쳐 있을 수도 있을 가능성 배제시키기 위함\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shpae[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.5.2 하이퍼파라미터 최적화</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `하이퍼파라미터 최적화`: 하이퍼파라미터의 '최적 값'이 존재하는 범위를 줄여나가는 방식  \n",
    "$\\equiv$ [ 대략적인 범위를 설정 $\\rightarrow$ 범위 내 무작위 샘플링 $\\rightarrow$ 해당 값으로 정확도 평가 ]의 과정 반복\n",
    "\n",
    "- `무작위 샘플링해 탐색하는 편`이 좋은 결과를 낸다고 알려져 있음  \n",
    "$\\equiv$ 최종 정확도에 미치는 영향력이 하이퍼파라미터마다 다른 특성 때문  \n",
    "cf) grid search: 규칙적인 탐색\n",
    "\n",
    "- 하이퍼파라미터의 범위는 '대략적으로' 지정하는 것이 효과적  \n",
    "ex) 보통 10의 계승 단위로(log scale) 범위 지정\n",
    "\n",
    "- `최적화 과정`  \n",
    "    A. 하이퍼파라미터 값의 범위 설정  \n",
    "    B. 설정된 범위 내 값을 무작위로 추출  \n",
    "    C. 추출된 값을 사용해 학습한 후, 검증 데이터로 정확도 평가(단, 적은 수의 에폭으로)  \n",
    "    D. B-C 단계를 특정 횟수만큼 반복해, 정확도의 결과로 하이퍼파라미터의 범위 축소  \n",
    "     이 후, 그 압축된 범위 내 값을 하나 골라냄  \n",
    "    \n",
    "cf) Bayesian optimization: 베이즈 정리를 중심으로 한 엄밀하고 효율적인 최적화 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.5.3 하이퍼파라미터 최적화 구현</h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST 데이터셋\n",
    "- 학습률과 가중치 감소 계수 탐색 문제\n",
    "\n",
    "- 하이퍼파라미터의 값을 $0.001 \\sim 1,000$ 같은 로그 스케일 범위 내 무작위 추출해 수행  \n",
    "$\\equiv$ 10 ** `np.random.uniform`(-3. 3)\n",
    "\n",
    "- 해당 구현에서는  \n",
    "    A. 가중치 감소 계수를 $10^{-8}\\sim 10^{-4}$  \n",
    "    B. 학습률을 $10^{-6}\\sim 10^{-2}$ 범위부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "lr = 10 ** np.random.uniform(-6, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.6 정리</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `매개변수 갱신` 방법: SGD(확률적 경사 하강법), 모멘텀, AdaGrad, Adam  \n",
    "- `가중치의 초깃값 설정` 방법: Xavier 초깃값, He 초깃값\n",
    "\n",
    "- `배치 정규화`: 빠른 학습 진행 가능, 초깃값에 의한 영향 축소\n",
    "\n",
    "- `오버피팅 억제 정규화` 기술: 가중치 감소, 드롭아웃  \n",
    "- `하이퍼파라미터 값 탐색`: 최적 값이 존재할 법한 범위 축소"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580025d11052cab5f63c2f4907fbf4dfa5dfc534c59c9143d0653120b93bb8e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
