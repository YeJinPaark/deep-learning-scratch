{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.4.2 가중치 감소</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `가중치 감소(weight decay)`: 학습 과정에서 큰 가중치에 대해서 그에 상응하는 큰 페널티를 부과하여 오버피팅을 억제하는 방법  \n",
    "- 신경망 학습의 목적: 손실 함수의 값을 줄이는 것\n",
    "\n",
    "A. `L2 법칙(가중치의 제곱 법칙)`  \n",
    "- 일반적으로 자주 쓰임  \n",
    "$${1 \\over 2} \\lambda W$$\n",
    "- 모든 가중치 각각의 손실 함수에 더함\n",
    "\n",
    "- $\\mathbf{\\lambda}$: 정규화의 세기를 조절하는 하이퍼파라미터  \n",
    "$\\equiv$ 크게 설정할수록 큰 가중치에 대한 페널티 부과  \n",
    "- ${1 \\over 2}$: 미분 결과 $\\lambda W$ 를 조절하는 상수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실험결과  \n",
    "    A. 훈련 데이터와 시험 데이터에 대한 정확도 차이가 비교적 감소  \n",
    "    B. 훈련 데이터에 대한 정확도가 1.0에 도달하지 못함\n",
    "\n",
    "- `장점`: 간단하게 구현 가능, 어느 정도 지나친 학습 억제 가능\n",
    "- `단점`: 복잡한 신경망 모델의 오버피팅 억제가 여려울 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. L1 법칙: 절댓값의 합  \n",
    "C. L $\\infty$ 법칙(Max 법칙): 각 원소의 절댓값 중 가장 큰 것에 해당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.4.3 드롭아웃(dropout)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. `훈련 때`  \n",
    "  - `뉴런을 무작위로 삭제하면서 학습하는 방법`  \n",
    "  - 삭제된 뉴런은 신호를 전달하지 않음\n",
    "\n",
    "B. `시험 때`  \n",
    "  - 모든 뉴런에 신호 전달  \n",
    "  - 각 뉴런의 출력에 훈련 때 삭제한 비율을 곱하여 출력  \n",
    "  - 실제로 딥러닝 프레임워크들에서 비율을 곱하지 않기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None                                                # 순전파 때, 삭제할 뉴런을 False로 표시\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio   # x와 같은 형상의 배열을 무작위로 생성-> 값이 dropout_ratio보다 큰 원소만 True로 설정\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "            return dout * self.mask                                     # ReLU와 동일한 동작 (순전파와 동일하게 작동)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실험결과\n",
    "    - 훈련 데이터와 시험 데이터에 대한 정확도 차이 감소\n",
    "    - 훈련 데이터에 대한 정확도 1.0에 도달 X\n",
    "    - 표현력을 높이면서도 오버피팅 억제 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `앙상블 학습(ensemble learning)`\n",
    "    - 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식\n",
    "\n",
    "    - ex) 신경망에서  \n",
    "        A. 훈련 때, 같거나 비슷한 구조의 네트워크를 n개 준비하여 따로 학습시킨 후,  \n",
    "        B. 시험 때, n개의 출력을 평균내어 답하는 방식  \n",
    "    - 신경망의 정확도 몇 %정도 개선한다는 사실 실험적으로 증명\n",
    "\n",
    "    - 드롭아웃과 밀접  \n",
    "        $\\equiv$ '학습 때, 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석한다' 라는 맥락에서 유사  \n",
    "        $\\equiv$ '추론 때, 뉴런의 출력에 삭제한 비율을 곱함으로써 여러 모델을 평균내는 것과 유사한 효과를 얻는다'\n",
    "        \n",
    "        $\\equiv$ 드롭아웃은 앙상블 학습과 같은 효과를 하나의 네트워크로 구현했다고 판단할 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
