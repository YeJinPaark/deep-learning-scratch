{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 3층 신경망 구현\n",
    "3.4.2 각 층의 신호 전달 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n"
     ]
    }
   ],
   "source": [
    "# 입력층 구현\n",
    "import numpy as np\n",
    "\n",
    "X=np.array([1.0, 0.5])\n",
    "W1=np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])\n",
    "B1=np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "A1=np.dot(X, W1) + B1\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "Z1=sigmoid(A1)\n",
    "\n",
    "print(A1)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n",
      "[0.62624937 0.7710107 ]\n"
     ]
    }
   ],
   "source": [
    "# 은닉층 구현\n",
    "\n",
    "W2=np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2=np.array([0.1, 0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2=np.dot(Z1, W2) + B2\n",
    "Z2=sigmoid(A2) # 은닉층의 활성화 함수\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "# 출력층 구현\n",
    "\n",
    "def identity_function(x): # 출력층의 활성화 함수(항등 함수)\n",
    "    return X\n",
    "\n",
    "W3=np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3=np.array([0.1, 0.2])\n",
    "\n",
    "A3=np.dot(Z2, W3) + B3\n",
    "Y=identity_function(A3)\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출력층의 활성화 함수\n",
    "    - 회귀: 항등 함수\n",
    "    - 2클래스 분류: sigmoid 함수\n",
    "    - 다중 클래스 분류: softmax 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.3 구현 정리\n",
    "    - 가중치만 대문자 표기, 편향과 중간 결과 소문자 표기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "def init_network(): # 각 층에 필요한 매개변수(가중치와 편향)을 딕셔너리 변수인 network에 저장\n",
    "    network={}\n",
    "    network['W1']=np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1']=np.array([0.1, 0.2, 0.3])\n",
    "    network['W2']=np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2']=np.array([0.1, 0.2])\n",
    "    network['W3']=np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3']=np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def identity_function(x): # 출력층의 활성화 함수(항등 함수)\n",
    "    return X\n",
    "\n",
    "def forward(network, x): # 입력 신호를 출력으로 변환하는 처리과정 구현, 수누전파(입력에서 출력 방향)\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1=np.dot(x, W1) + b1\n",
    "    z1=sigmoid(a1)\n",
    "    # print(z1)\n",
    "    a2=np.dot(z1, W2) + b2\n",
    "    z2=sigmoid(a2)\n",
    "    # print(z2)\n",
    "    a3=np.dot(z2, W3) + b3\n",
    "    # print(a3)\n",
    "    y=identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "def identity_function(x): # 출력층의 활성화 함수(항등 함수)\n",
    "    return x\n",
    "\n",
    "network=init_network()\n",
    "x=np.array([1.0, 0.5])\n",
    "y=forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 출력층 설계\n",
    "    - 회귀(입력 데이터에서 연속적인 수치를 예측): 항등 함수\n",
    "    - 분류(데이터가 어느 class에 속하는지): softmax 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1 항등 함수와 소프트맥스 함수 구현\n",
    "    - 항등 함수: 입력 신호가 그대로 출력\n",
    "    - softmax 함수: yk=exp(ak)/1~n까지의 exp(ai)\n",
    "        - softmax의 출력은 모든 입력 신호로부터 영향을 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n",
      "74.1221542101633\n",
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0.3, 2.9, 4.0])\n",
    "exp_a=np.exp(a)\n",
    "print(exp_a)\n",
    "\n",
    "sum_exp_a=np.sum(exp_a)\n",
    "print(sum_exp_a)\n",
    "\n",
    "y=exp_a/sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "# 함수화\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a=np.exp(a)\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "a=np.array([0.3, 2.9, 4.0])\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.2 softmax 함수 구현 시 주의점\n",
    "    - 위 함수로 구현 시, 지수 함수 연산으로 인한 오버플로 문제 발생\n",
    "    = e^1000는 inf(무한대)\n",
    "    - softmax 연산 시, 어떤 정수를 더하여도 결과는 변하지 않음\n",
    "    -> 입력 신호의 최대값을 이용하여 오버플로 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan]\n",
      "[  0 -10 -20]\n",
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a2322\\AppData\\Local\\Temp\\ipykernel_10808\\3587206884.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(a)/np.sum(np.exp(a))) # 오버플로 발생\n",
      "C:\\Users\\a2322\\AppData\\Local\\Temp\\ipykernel_10808\\3587206884.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  print(np.exp(a)/np.sum(np.exp(a))) # 오버플로 발생\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.array([1010, 1000, 990])\n",
    "print(np.exp(a)/np.sum(np.exp(a))) # 오버플로 발생\n",
    "\n",
    "c=np.max(a)\n",
    "print(a-c) # 오버플로 방지\n",
    "\n",
    "print(np.exp(a-c)/np.sum(np.exp(a-c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n"
     ]
    }
   ],
   "source": [
    "def softmax(a):\n",
    "    c=np.max(a)\n",
    "    exp_a=np.exp(a-c) # 오버플로 방지\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "a=np.array([1010, 1000, 990])\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.3 softmax 함수의 특징\n",
    "    - softmax 함수 출력의 총합은 1-> 확률로 해석\n",
    "    - softmax를 적용해도 각 원소의 대소 관계는 변하지 않음(지수 함수가 단조 증가 함수이기 때문) = 입력의 대소 관계가 출력에서도 유지\n",
    "    = 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 softmax 함수는 생략하는 것이 일반적(ㅊ론 단계에서, 학습 단계에서는 생략 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a=np.array([0.3, 2.9, 4.0])\n",
    "y=softmax(a)\n",
    "print(y)\n",
    "print(np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.4 출력층의 뉴런 수 정하기\n",
    "    - 분류: 분류하고자 하는 class 수로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6 손글씨 숫자 인식\n",
    "    - 추론 과정, 순전파(forward propagation)\n",
    "    - 신경망: 훈련 데이터로 가중치 매개변수 학습-> 추론 단계에서 해당 매개변수로 입력 데이터 분류\n",
    "\n",
    "3.6.1 MNIST 데이터셋\n",
    "    - 손글씨 숫자 이미지 데이터셋\n",
    "    - 0~9까지의 숫자 이미지로 구성\n",
    "    - 6,000장 훈련 이미지 + 10,000장 시험 이미지\n",
    "    - 28*28 크기의 회색조 이미지(1채널)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from mnist import load_mnist\n",
    "\n",
    "# (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블) 형식으로 반환\n",
    "(x_train, t_train), (x_test, t_test)=load_mnist(flatten=True, normalize=False)\n",
    "# normalize: 입력 이미지의 픽셀값을 0.0~1.0 사이의 값으로 정규화할 지 결정 - False: 원래 값 그대로 0~255 유지\n",
    "# flatten: 입력 이미지를 평탄하게, 즉 1차원 배열로 만들지 결정- False: 1*28*28의 3차원 배열, True: 784개의 원소로 이뤄진 1차원 배열\n",
    "# one_hot_label: 원-핫 인코딩 형태로 저장할지 결정 - False: 숫자 형태의 레이블로 저장\n",
    "\n",
    "# 각 데이터의 형상 출력\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터셋 확인\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img=Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "# flatten=True == 1차원 넘파이 배열 형태로 이미지 저장\n",
    "\n",
    "img=x_train[0]\n",
    "label=t_train[0]\n",
    "print(label)\n",
    "\n",
    "print(img.shape)\n",
    "img=img.reshape(28, 28) # 원래 이미지 모양으로 변형, 원래 형상인 28*28 크기로 복구\n",
    "print(img.shape)\n",
    "\n",
    "img_show(img) # 내부의 Image.fromarray(): 넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28435c4127f67b7607031f32967fe3943e6f69f1a5ec9adab2b1e0962cf71e07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
